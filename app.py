from flask import Flask, render_template, request, jsonify, send_file, Response
from googleapiclient.discovery import build
from datetime import datetime, timedelta
import os
from dotenv import load_dotenv
import isodate
import csv
from io import StringIO

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

app = Flask(__name__)

# YouTube API è¨­å®š
YOUTUBE_API_SERVICE_NAME = 'youtube'
YOUTUBE_API_VERSION = 'v3'

# å„²å­˜æœ€æ–°æœå°‹çµæœç”¨æ–¼åŒ¯å‡º
last_search_results = []
# å„²å­˜æœå°‹æ¢ä»¶ç”¨æ–¼æª”æ¡ˆå‘½å
last_search_params = {}

# çœŸå¯¦çš„ API é…é¡è¿½è¹¤ï¼ˆæ¯æ—¥ç´¯ç©ï¼‰
daily_quota_usage = {
    'date': datetime.now().strftime('%Y-%m-%d'),
    'search_calls': 0,
    'video_calls': 0,
    'category_calls': 0,
    'total_cost': 0
}

def get_api_key():
    """ç²å– API Keyï¼Œæ¯æ¬¡éƒ½é‡æ–°è®€å–ç’°å¢ƒè®Šæ•¸"""
    # é‡æ–°è¼‰å…¥ç’°å¢ƒè®Šæ•¸ä»¥ç¢ºä¿æœ€æ–°å€¼
    load_dotenv(override=True)
    api_key = os.getenv('YOUTUBE_API_KEY')
    print(f"ğŸ” è®€å–åˆ°çš„ API Key: {api_key[:15] + '...' if api_key and len(api_key) > 15 else api_key}")
    return api_key

def get_youtube_service():
    """å»ºç«‹ YouTube API æœå‹™"""
    api_key = get_api_key()
    
    if not api_key or api_key == 'your_youtube_api_key_here':
        raise ValueError("è«‹è¨­å®šæœ‰æ•ˆçš„ YouTube API Key")
    
    # æª¢æŸ¥ API Key æ ¼å¼
    if not api_key.startswith('AIzaSy') or len(api_key) != 39:
        raise ValueError(f"API Key æ ¼å¼éŒ¯èª¤ã€‚YouTube API Key æ‡‰è©²ä»¥ 'AIzaSy' é–‹é ­ä¸”é•·åº¦ç‚º 39 å­—ç¬¦ã€‚ç›®å‰çš„ Key: {api_key[:10]}...")
    
    print(f"ğŸ”‘ ä½¿ç”¨ API Key: {api_key[:15]}...{api_key[-5:]}")
    return build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=api_key)

def get_time_filter(days):
    """æ ¹æ“šå¤©æ•¸ç”¢ç”Ÿæ™‚é–“éæ¿¾å™¨"""
    if days == 1:
        published_after = datetime.utcnow() - timedelta(days=1)
    elif days == 3:
        published_after = datetime.utcnow() - timedelta(days=3)
    elif days == 5:
        published_after = datetime.utcnow() - timedelta(days=5)
    elif days == 7:
        published_after = datetime.utcnow() - timedelta(days=7)
    else:
        return None

    return published_after.strftime('%Y-%m-%dT%H:%M:%SZ')

def get_time_segments(days):
    """
    å°‡æ™‚é–“ç¯„åœåˆ†æˆå¤šå€‹æ™‚é–“æ®µï¼Œç”¨æ–¼åˆ†æ®µæœå°‹
    è¿”å›æ ¼å¼: [(start_time, end_time), ...]
    start_time å’Œ end_time éƒ½æ˜¯ ISO 8601 æ ¼å¼å­—ä¸²
    """
    if not days or days == 'all':
        return []

    days = int(days)
    now = datetime.utcnow()
    segments = []

    # å°‡æ™‚é–“ç¯„åœåˆ†æˆ 3 æ®µ
    if days <= 2:
        # 1-2å¤©ï¼šä¸åˆ†æ®µï¼Œç›´æ¥æœå°‹
        start = now - timedelta(days=days)
        segments.append((start.strftime('%Y-%m-%dT%H:%M:%SZ'), None))
    elif days <= 5:
        # 3-5å¤©ï¼šåˆ†æˆ 3 æ®µ
        segment_days = days / 3
        for i in range(3):
            start = now - timedelta(days=(i+1) * segment_days)
            end = now - timedelta(days=i * segment_days) if i > 0 else None
            segments.append((start.strftime('%Y-%m-%dT%H:%M:%SZ'), end.strftime('%Y-%m-%dT%H:%M:%SZ') if end else None))
    else:
        # 7å¤©ä»¥ä¸Šï¼šåˆ†æˆ 3 æ®µ
        segment_days = days / 3
        for i in range(3):
            start = now - timedelta(days=(i+1) * segment_days)
            end = now - timedelta(days=i * segment_days) if i > 0 else None
            segments.append((start.strftime('%Y-%m-%dT%H:%M:%SZ'), end.strftime('%Y-%m-%dT%H:%M:%SZ') if end else None))

    return segments

def format_duration(duration):
    """å°‡ ISO 8601 æ ¼å¼çš„æ™‚é–“è½‰æ›ç‚ºå¯è®€æ ¼å¼"""
    try:
        parsed_duration = isodate.parse_duration(duration)
        total_seconds = int(parsed_duration.total_seconds())
        hours = total_seconds // 3600
        minutes = (total_seconds % 3600) // 60
        seconds = total_seconds % 60
        
        if hours > 0:
            return f"{hours}:{minutes:02d}:{seconds:02d}"
        else:
            return f"{minutes}:{seconds:02d}"
    except:
        return duration

def get_duration_seconds(duration):
    """å°‡ ISO 8601 æ ¼å¼çš„æ™‚é–“è½‰æ›ç‚ºç§’æ•¸"""
    try:
        parsed_duration = isodate.parse_duration(duration)
        return int(parsed_duration.total_seconds())
    except:
        return 0

def format_view_count(view_count):
    """æ ¼å¼åŒ–è§€çœ‹æ¬¡æ•¸"""
    try:
        count = int(view_count)
        if count >= 1000000:
            return f"{count/1000000:.1f}M"
        elif count >= 1000:
            return f"{count/1000:.1f}K"
        else:
            return str(count)
    except:
        return view_count

def get_video_categories(youtube, region_code='TW'):
    """ç²å–YouTubeå½±ç‰‡é¡åˆ¥æ¸…å–®"""
    try:
        categories_response = youtube.videoCategories().list(
            part='snippet',
            regionCode=region_code
        ).execute()
        
        categories = {}
        for item in categories_response['items']:
            categories[item['id']] = item['snippet']['title']
        
        return categories
    except Exception as e:
        print(f"âš ï¸  ç„¡æ³•ç²å–é¡åˆ¥è³‡è¨Š: {e}")
        return {}

def get_category_name(category_id, categories_dict):
    """æ ¹æ“šé¡åˆ¥IDç²å–é¡åˆ¥åç¨±"""
    return categories_dict.get(category_id, f'æœªçŸ¥é¡åˆ¥ ({category_id})')

def update_quota_usage(search_calls=0, video_calls=0, category_calls=0):
    """æ›´æ–°çœŸå¯¦çš„ API é…é¡ä½¿ç”¨é‡"""
    global daily_quota_usage
    
    # æª¢æŸ¥æ˜¯å¦æ˜¯æ–°çš„ä¸€å¤©ï¼Œå¦‚æœæ˜¯å‰‡é‡ç½®
    today = datetime.now().strftime('%Y-%m-%d')
    if daily_quota_usage['date'] != today:
        print(f"ğŸ—“ï¸ æ–°çš„ä¸€å¤©é–‹å§‹ï¼Œé‡ç½®é…é¡è¨ˆç®—")
        daily_quota_usage = {
            'date': today,
            'search_calls': 0,
            'video_calls': 0,
            'category_calls': 0,
            'total_cost': 0
        }
    
    # ç´¯ç©ä½¿ç”¨é‡
    daily_quota_usage['search_calls'] += search_calls
    daily_quota_usage['video_calls'] += video_calls
    daily_quota_usage['category_calls'] += category_calls
    
    # è¨ˆç®—ç¸½æˆæœ¬ï¼ˆæœå°‹ 100 å–®ä½ï¼Œå…¶ä»– 1 å–®ä½ï¼‰
    total_cost = (daily_quota_usage['search_calls'] * 100 + 
                  daily_quota_usage['video_calls'] * 1 + 
                  daily_quota_usage['category_calls'] * 1)
    daily_quota_usage['total_cost'] = total_cost
    
    print(f"ğŸ“Š é…é¡æ›´æ–°: æœå°‹{daily_quota_usage['search_calls']}æ¬¡, å½±ç‰‡{daily_quota_usage['video_calls']}å€‹, é¡åˆ¥{daily_quota_usage['category_calls']}æ¬¡ = {total_cost}å–®ä½")
    
    return daily_quota_usage

def get_current_quota_info():
    """ç²å–ç•¶å‰é…é¡è³‡è¨Š"""
    global daily_quota_usage
    
    # æª¢æŸ¥æ—¥æœŸ
    today = datetime.now().strftime('%Y-%m-%d')
    if daily_quota_usage['date'] != today:
        update_quota_usage()  # é‡ç½®åˆ°æ–°ä¸€å¤©
    
    total_cost = daily_quota_usage['total_cost']
    remaining_quota = 10000 - total_cost
    
    # è¨ˆç®—é‚„èƒ½åšå¹¾æ¬¡æœå°‹ï¼ˆå‡è¨­æ¯æ¬¡æœå°‹ 25 å€‹çµæœ = 125 å–®ä½ï¼‰
    estimated_searches_left = max(0, remaining_quota // 125)
    
    return {
        'current_cost': total_cost,
        'remaining_quota': remaining_quota,
        'estimated_searches_left': estimated_searches_left,
        'quota_percentage': round((total_cost / 10000) * 100, 1),
        'video_count': 0,  # é€™å€‹æœƒåœ¨èª¿ç”¨æ™‚æ›´æ–°
        'search_calls': daily_quota_usage['search_calls'],
        'video_calls': daily_quota_usage['video_calls'],
        'category_calls': daily_quota_usage['category_calls']
    }

def calculate_quota_cost(search_count=0, video_details_count=0, categories_call=0):
    """è¨ˆç®— API é…é¡æ¶ˆè€—
    
    YouTube Data API v3 é…é¡æ¶ˆè€—:
    - search().list(): 100 å–®ä½
    - videos().list(): 1 å–®ä½ (æ¯å€‹å½±ç‰‡)
    - videoCategories().list(): 1 å–®ä½
    """
    search_cost = search_count * 100
    video_details_cost = video_details_count * 1
    categories_cost = categories_call * 1
    
    total_cost = search_cost + video_details_cost + categories_cost
    return {
        'search_cost': search_cost,
        'video_details_cost': video_details_cost,
        'categories_cost': categories_cost,
        'total_cost': total_cost,
        'remaining_quota': 10000 - total_cost  # å‡è¨­æ¯æ—¥é…é¡ 10,000
    }

def format_quota_info(quota_info, video_count):
    """æ ¼å¼åŒ–é…é¡è³‡è¨Šç‚ºç”¨æˆ¶å‹å–„çš„è¨Šæ¯"""
    total_cost = quota_info['total_cost']
    remaining = quota_info['remaining_quota']
    
    # è¨ˆç®—é‚„èƒ½åšå¹¾æ¬¡æœå°‹
    estimated_searches_left = remaining // 125  # å‡è¨­æ¯æ¬¡æœå°‹ 25 å€‹çµæœ
    
    return {
        'current_cost': total_cost,
        'remaining_quota': remaining,
        'estimated_searches_left': max(0, estimated_searches_left),
        'quota_percentage': round((total_cost / 10000) * 100, 1),
        'video_count': video_count
    }

def generate_excel_filename(search_params):
    """æ ¹æ“šæœå°‹æ¢ä»¶ç”ŸæˆExcelæª”æ¡ˆåç¨±"""
    # é˜²éŒ¯è™•ç†
    if not search_params:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        return f'YouTube_æœå°‹çµæœ_{timestamp}.xlsx'
    
    parts = []
    
    # é—œéµå­—ï¼ˆä¸é¡¯ç¤ºé è¨­çš„ shortsï¼‰
    keyword = search_params.get('keyword', 'shorts')
    if keyword and keyword != 'shorts':
        # æ¸…ç†ä¸åˆæ³•çš„æª”æ¡ˆåå­—å…ƒ
        safe_keyword = ''.join(c for c in keyword if c.isalnum() or c in '-_')
        if safe_keyword:  # ç¢ºä¿æ¸…ç†å¾Œé‚„æœ‰å…§å®¹
            parts.append(safe_keyword[:20])  # é™åˆ¶é•·åº¦
    
    # é¡åˆ¥
    category_filter = search_params.get('category_filter', 'all')
    if category_filter != 'all':
        category_names = {
            '1': 'é›»å½±', '2': 'æ±½è»Š', '10': 'éŸ³æ¨‚', '15': 'å¯µç‰©',
            '17': 'é«”è‚²', '19': 'æ—…éŠ', '20': 'éŠæˆ²', '22': 'éƒ¨è½æ ¼',
            '23': 'å–œåŠ‡', '24': 'å¨›æ¨‚', '25': 'æ–°è', '26': 'æ•™å­¸',
            '27': 'æ•™è‚²', '28': 'ç§‘æŠ€'
        }
        category_name = category_names.get(category_filter, f'cat{category_filter}')
        parts.append(category_name)
    
    # ä¸Šå‚³æ™‚é–“
    time_filter = search_params.get('time_filter', 'all')
    if time_filter != 'all':
        parts.append(f'{time_filter}h')
    
    # æœ€å°‘è§€çœ‹æ¬¡æ•¸
    min_views = search_params.get('min_views', 0)
    if min_views > 0:
        if min_views >= 1000000:
            parts.append(f'{min_views//1000000}Mè§€çœ‹')
        elif min_views >= 1000:
            parts.append(f'{min_views//1000}Kè§€çœ‹')
        else:
            parts.append(f'{min_views}è§€çœ‹')
    
    # å½±ç‰‡é•·åº¦
    max_duration = search_params.get('max_duration', 'all')
    if max_duration != 'all':
        parts.append(f'{max_duration}s')
    
    # çµæœæ•¸é‡ï¼ˆåªåœ¨éé è¨­å€¼æ™‚é¡¯ç¤ºï¼‰
    max_results = search_params.get('max_results', 25)
    if max_results != 25:
        parts.append(f'{max_results}ç­†')
    
    # æ™‚é–“æˆ³
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # çµ„åˆæª”æ¡ˆåï¼ˆéæ¿¾ç©ºéƒ¨ä»¶ï¼‰
    if parts:
        filename_parts = '_'.join(parts)
        filename = f'YouTube_{filename_parts}_{timestamp}.xlsx'
    else:
        filename = f'YouTubeæœå°‹_{timestamp}.xlsx'
    
    # ç¢ºä¿æª”æ¡ˆåä¸éé•·
    if len(filename) > 100:
        filename = f'YouTube_æœå°‹çµæœ_{timestamp}.xlsx'
    
    return filename

def generate_csv_filename(search_params):
    """æ ¹æ“šæœå°‹æ¢ä»¶ç”ŸæˆCSVæª”æ¡ˆåç¨±"""
    # é˜²éŒ¯è™•ç†
    if not search_params:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        return f'YouTube_Search_{timestamp}.csv'
    
    parts = []
    
    # é—œéµå­—ï¼ˆä¸é¡¯ç¤ºé è¨­çš„ shortsï¼‰
    keyword = search_params.get('keyword', 'shorts')
    if keyword and keyword != 'shorts':
        # æ¸…ç†ä¸åˆæ³•çš„æª”æ¡ˆåå­—å…ƒï¼Œåªä¿ç•™è‹±æ•¸å­—
        safe_keyword = ''.join(c for c in keyword if c.isalnum() or c in '-_')
        if safe_keyword:  # ç¢ºä¿æ¸…ç†å¾Œé‚„æœ‰å…§å®¹
            parts.append(safe_keyword[:20])  # é™åˆ¶é•·åº¦
    
    # é¡åˆ¥ - ä½¿ç”¨è‹±æ–‡é¿å…ç·¨ç¢¼å•é¡Œ
    category_filter = search_params.get('category_filter', 'all')
    if category_filter != 'all':
        category_names = {
            '1': 'movie', '2': 'autos', '10': 'music', '15': 'pets',
            '17': 'sports', '19': 'travel', '20': 'gaming', '22': 'blogs',
            '23': 'comedy', '24': 'entertainment', '25': 'news', '26': 'howto',
            '27': 'education', '28': 'tech'
        }
        category_name = category_names.get(category_filter, f'cat{category_filter}')
        parts.append(category_name)
    
    # ä¸Šå‚³æ™‚é–“
    time_filter = search_params.get('time_filter', 'all')
    if time_filter != 'all':
        parts.append(f'{time_filter}h')
    
    # æœ€å°‘è§€çœ‹æ¬¡æ•¸ - ç§»é™¤ä¸­æ–‡
    min_views = search_params.get('min_views', 0)
    if min_views > 0:
        if min_views >= 1000000:
            parts.append(f'{min_views//1000000}M')
        elif min_views >= 1000:
            parts.append(f'{min_views//1000}K')
        else:
            parts.append(f'{min_views}v')
    
    # å½±ç‰‡é•·åº¦
    max_duration = search_params.get('max_duration', 'all')
    if max_duration != 'all':
        parts.append(f'{max_duration}s')
    
    # çµæœæ•¸é‡ï¼ˆåªåœ¨éé è¨­å€¼æ™‚é¡¯ç¤ºï¼‰
    max_results = search_params.get('max_results', 25)
    if max_results != 25:
        parts.append(f'{max_results}results')
    
    # æ™‚é–“æˆ³
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # çµ„åˆæª”æ¡ˆåï¼ˆéæ¿¾ç©ºéƒ¨ä»¶ï¼Œåªä½¿ç”¨è‹±æ•¸å­—ï¼‰
    if parts:
        filename_parts = '_'.join(parts)
        filename = f'YouTube_{filename_parts}_{timestamp}.csv'
    else:
        filename = f'YouTube_Search_{timestamp}.csv'
    
    # ç¢ºä¿æª”æ¡ˆåä¸éé•·
    if len(filename) > 100:
        filename = f'YouTube_Search_{timestamp}.csv'
    
    return filename

@app.route('/')
def index():
    """é¦–é """
    return render_template('index.html')

@app.route('/search', methods=['POST'])
def search_videos():
    """æœå°‹å½±ç‰‡"""
    global last_search_results, last_search_params  # åœ¨å‡½æ•¸é–‹é ­çµ±ä¸€å®£å‘Š
    
    try:
        # å–å¾—æœå°‹åƒæ•¸
        data = request.get_json()
        keyword = data.get('keyword', '').strip()
        category_filter = data.get('categoryFilter', 'all')
        region_filter = data.get('regionFilter', 'all')  # é è¨­ä¸æŒ‡å®šåœ°å€
        time_filter = data.get('timeFilter', 'all')
        min_views = data.get('minViews', 500000)  # é è¨­æœ€å°‘è§€çœ‹æ¬¡æ•¸50è¬
        max_duration = data.get('maxDuration', 'all')
        max_results = data.get('maxResults', 25)
        
        # å„²å­˜æœå°‹æ¢ä»¶ç”¨æ–¼æª”æ¡ˆå‘½å
        last_search_params = {
            'keyword': keyword if keyword else 'shorts',
            'category_filter': category_filter,
            'region_filter': region_filter,
            'time_filter': time_filter,
            'min_views': min_views,
            'max_duration': max_duration,
            'max_results': max_results
        }
        
        # å¦‚æœæ²’æœ‰é—œéµå­—ï¼Œä½¿ç”¨é è¨­çš„ shorts é—œéµå­—
        if not keyword:
            keyword = 'shorts'
            print("ğŸ¥ ä½¿ç”¨é è¨­é—œéµå­—: shorts")
        
        print(f"ğŸ” æœå°‹åƒæ•¸: é—œéµå­—='{keyword}', é¡åˆ¥={category_filter}, åœ°å€={region_filter}, æ™‚é–“={time_filter}, æœ€å°‘è§€çœ‹={min_views}, æœ€å¤§é•·åº¦={max_duration}")
        
        # å»ºç«‹ YouTube æœå‹™
        youtube = get_youtube_service()
        
        # ç²å–å½±ç‰‡é¡åˆ¥è³‡è¨Š
        print("ğŸ“Š ç²å–å½±ç‰‡é¡åˆ¥è³‡è¨Š...")
        categories = get_video_categories(youtube)
        print(f"ğŸ·ï¸  ç²å–åˆ° {len(categories)} å€‹é¡åˆ¥")
        
        # æ›´æ–°é…é¡ä½¿ç”¨ï¼ˆé¡åˆ¥ API èª¿ç”¨ï¼‰
        update_quota_usage(category_calls=1)
        
        # è¨˜éŒ„ API å‘¼å«æ¬¡æ•¸ç”¨æ–¼é…é¡è¨ˆç®—
        api_calls = {
            'search_count': 0,
            'video_details_count': 0,
            'categories_call': 1  # ç²å–é¡åˆ¥è³‡è¨Š
        }
        
        print(f"ğŸ” ä½¿ç”¨é—œéµå­—æœå°‹: {keyword}")
        print(f"ğŸ¯ ä½¿ç”¨æ’åºæ–¹å¼: viewCount (è§€çœ‹æ•¸æœ€é«˜)")

        # ç‚ºäº†ç²å¾—è¶³å¤ çš„çµæœï¼Œæˆ‘å€‘å…ˆæœå°‹æ›´å¤šçš„å½±ç‰‡
        search_batch_size = 50  # YouTube API å–®æ¬¡æœ€å¤§å€¼

        # åŸºç¤æœå°‹åƒæ•¸
        base_search_params = {
            'part': 'snippet',
            'q': keyword,
            'type': 'video',
            'order': 'viewCount',  # å›ºå®šä½¿ç”¨è§€çœ‹æ•¸æ’åºï¼Œç¢ºä¿æ‰¾åˆ°æœ€ç†±é–€å½±ç‰‡
            'maxResults': search_batch_size,
            'videoDuration': 'short'  # é™åˆ¶ç‚ºçŸ­å½±ç‰‡ï¼ˆ< 4 åˆ†é˜ï¼‰ï¼Œæå‡ Shorts æœå°‹å‘½ä¸­ç‡
        }

        # åªæœ‰åœ¨æŒ‡å®šåœ°å€æ™‚æ‰åŠ å…¥ regionCode åƒæ•¸
        if region_filter and region_filter != 'all':
            base_search_params['regionCode'] = region_filter
            print(f"ğŸŒ ä½¿ç”¨åœ°å€éæ¿¾: {region_filter}")
        else:
            print(f"ğŸŒ ä¸é™åˆ¶åœ°å€")

        # æ ¹æ“šåœ°å€è¨­å®šèªè¨€åå¥½ï¼ˆåªåœ¨æœ‰æŒ‡å®šåœ°å€æ™‚ï¼‰
        if region_filter and region_filter != 'all':
            if region_filter in ['TW', 'CN', 'HK', 'SG']:
                base_search_params['relevanceLanguage'] = 'zh'
            elif region_filter == 'JP':
                base_search_params['relevanceLanguage'] = 'ja'
            elif region_filter == 'KR':
                base_search_params['relevanceLanguage'] = 'ko'
            elif region_filter in ['NO']:
                base_search_params['relevanceLanguage'] = 'no'
            elif region_filter in ['CH', 'DE']:
                base_search_params['relevanceLanguage'] = 'de'
            elif region_filter in ['DK']:
                base_search_params['relevanceLanguage'] = 'da'
            elif region_filter in ['AE', 'SA']:
                base_search_params['relevanceLanguage'] = 'ar'
            elif region_filter in ['US', 'GB', 'CA', 'AU', 'IN']:
                base_search_params['relevanceLanguage'] = 'en'
            elif region_filter in ['FR']:
                base_search_params['relevanceLanguage'] = 'fr'
            elif region_filter in ['RU']:
                base_search_params['relevanceLanguage'] = 'ru'

        # æ·»åŠ åˆ†é¡éæ¿¾å™¨
        if category_filter != 'all':
            base_search_params['videoCategoryId'] = category_filter
            print(f"ğŸ·ï¸  ä½¿ç”¨åˆ†é¡éæ¿¾: {category_filter}")

        # ç²å–æ™‚é–“åˆ†æ®µ
        time_segments = get_time_segments(time_filter) if time_filter != 'all' else []

        # æ”¶é›†æ‰€æœ‰å½±ç‰‡IDä¸¦å»é™¤é‡è¤‡
        all_video_ids = []
        seen_ids = set()

        if time_segments:
            # ä½¿ç”¨æ™‚é–“åˆ†æ®µæœå°‹ç­–ç•¥
            print(f"â° ä½¿ç”¨æ™‚é–“åˆ†æ®µæœå°‹: {len(time_segments)} å€‹æ™‚é–“æ®µ")
            for idx, (start_time, end_time) in enumerate(time_segments):
                search_params = base_search_params.copy()
                search_params['publishedAfter'] = start_time
                if end_time:
                    search_params['publishedBefore'] = end_time

                time_range = f"{start_time[:10]} åˆ° {end_time[:10] if end_time else 'ç¾åœ¨'}"
                print(f"ğŸ“… æ™‚é–“æ®µ {idx+1}/{len(time_segments)}: {time_range}")

                # åŸ·è¡Œæœå°‹
                search_response = youtube.search().list(**search_params).execute()
                api_calls['search_count'] += 1
                update_quota_usage(search_calls=1)

                # æ”¶é›†å½±ç‰‡ID
                segment_count = 0
                for item in search_response['items']:
                    video_id = item['id']['videoId']
                    if video_id not in seen_ids:
                        all_video_ids.append(video_id)
                        seen_ids.add(video_id)
                        segment_count += 1

                print(f"   âœ… è©²æ™‚é–“æ®µæ‰¾åˆ° {segment_count} å€‹æ–°å½±ç‰‡")
        else:
            # æ²’æœ‰æ™‚é–“éæ¿¾æˆ–æ™‚é–“éæ¿¾ç‚º 'all'ï¼ŒåŸ·è¡Œä¸€èˆ¬æœå°‹ä¸¦ç¿»é 
            print(f"ğŸ” åŸ·è¡Œä¸€èˆ¬æœå°‹ï¼ˆç„¡æ™‚é–“é™åˆ¶ï¼‰")
            search_params = base_search_params.copy()

            # åŸ·è¡Œåˆæ¬¡æœå°‹
            search_response = youtube.search().list(**search_params).execute()
            api_calls['search_count'] = 1
            update_quota_usage(search_calls=1)

            # æ”¶é›†å½±ç‰‡ID
            for item in search_response['items']:
                video_id = item['id']['videoId']
                if video_id not in seen_ids:
                    all_video_ids.append(video_id)
                    seen_ids.add(video_id)

            # å¦‚æœçµæœä¸å¤ ï¼Œå˜—è©¦ç²å–ä¸‹ä¸€é ï¼ˆæœ€å¤š 150 æ”¯ï¼‰
            while len(all_video_ids) < 150 and 'nextPageToken' in search_response:
                print(f"ğŸ“„ ç•¶å‰æœ‰ {len(all_video_ids)} å€‹å½±ç‰‡ï¼Œå˜—è©¦ç²å–æ›´å¤š...")
                search_params['pageToken'] = search_response['nextPageToken']
                search_response = youtube.search().list(**search_params).execute()
                api_calls['search_count'] += 1
                update_quota_usage(search_calls=1)

                for item in search_response['items']:
                    video_id = item['id']['videoId']
                    if video_id not in seen_ids and len(all_video_ids) < 150:
                        all_video_ids.append(video_id)
                        seen_ids.add(video_id)

                # é¿å…éå¤šAPIå‘¼å«ï¼ˆæœ€å¤š 3 æ¬¡ç¿»é ï¼‰
                if api_calls['search_count'] >= 3:
                    break
        
        print(f"ğŸ¥ ç¸½å…±ç²å–åˆ° {len(all_video_ids)} å€‹å”¯ä¸€å½±ç‰‡ ID")
        
        if not all_video_ids:
            print("âš ï¸  æ²’æœ‰ç²å–åˆ°ä»»ä½•å½±ç‰‡ ID")
            return jsonify({
                'success': True,
                'videos': [],
                'totalResults': 0
            })
        
        # åˆ†æ‰¹å–å¾—å½±ç‰‡è©³ç´°è³‡è¨Šï¼ˆYouTube API é™åˆ¶æ¯æ¬¡æœ€å¤š50å€‹IDï¼‰
        batch_size = 50
        all_video_items = []
        
        for i in range(0, len(all_video_ids), batch_size):
            batch_ids = all_video_ids[i:i + batch_size]
            print(f"ğŸ“Š è™•ç†ç¬¬ {i//batch_size + 1} æ‰¹å½±ç‰‡ï¼Œå…± {len(batch_ids)} å€‹")
            
            try:
                videos_response = youtube.videos().list(
                    part='snippet,statistics,contentDetails',
                    id=','.join(batch_ids)
                ).execute()
                
                all_video_items.extend(videos_response.get('items', []))
                # è¨˜éŒ„å¯¦éš›è™•ç†çš„å½±ç‰‡æ•¸é‡ç”¨æ–¼é…é¡è¨ˆç®—
                api_calls['video_details_count'] += len(batch_ids)
                
                # æ›´æ–°é…é¡ä½¿ç”¨ï¼ˆå½±ç‰‡è©³æƒ… API èª¿ç”¨ï¼‰
                update_quota_usage(video_calls=len(batch_ids))
                
            except Exception as e:
                print(f"âš ï¸ æ‰¹æ¬¡ {i//batch_size + 1} è™•ç†å¤±æ•—: {e}")
                continue
        
        print(f"ğŸ“Š æˆåŠŸç²å–åˆ° {len(all_video_items)} å€‹å½±ç‰‡è©³ç´°è³‡è¨Š")

        videos = []
        processed_video_ids = set()  # ç”¨æ–¼å»é‡

        for item in all_video_items:
            video_data = item['snippet']
            statistics = item['statistics']
            content_details = item['contentDetails']
            video_id = item['id']

            # æª¢æŸ¥æ˜¯å¦å·²è™•ç†éæ­¤å½±ç‰‡ï¼ˆå»é‡ï¼‰
            if video_id in processed_video_ids:
                print(f"â­ï¸  è·³éé‡è¤‡å½±ç‰‡ {video_id}")
                continue

            # æª¢æŸ¥è§€çœ‹æ¬¡æ•¸éæ¿¾å™¨
            view_count = int(statistics.get('viewCount', 0))
            if view_count < min_views:
                print(f"â­ï¸  è·³éå½±ç‰‡ {video_id}: è§€çœ‹æ¬¡æ•¸ {view_count} < {min_views}")
                continue

            # æª¢æŸ¥å½±ç‰‡é•·åº¦éæ¿¾å™¨
            if max_duration != 'all':
                duration_seconds = get_duration_seconds(content_details.get('duration', ''))
                max_duration_seconds = int(max_duration)
                if duration_seconds > max_duration_seconds:
                    print(f"â­ï¸  è·³éå½±ç‰‡ {video_id}: é•·åº¦ {duration_seconds}ç§’ > {max_duration_seconds}ç§’")
                    continue

            print(f"âœ… åŒ…å«å½±ç‰‡ {video_id}: {video_data['title'][:50]}... (è§€çœ‹: {view_count}, é•·åº¦: {get_duration_seconds(content_details.get('duration', ''))}ç§’)")

            # ç²å–é¡åˆ¥åç¨±
            category_id = video_data.get('categoryId', '')
            category_name = get_category_name(category_id, categories)

            video_info = {
                'videoId': video_id,
                'title': video_data['title'],
                'description': video_data['description'],
                'channelTitle': video_data['channelTitle'],
                'channelId': video_data['channelId'],
                'publishedAt': video_data['publishedAt'],
                'thumbnails': video_data['thumbnails'],
                'categoryId': category_id,
                'categoryName': category_name,
                'defaultLanguage': video_data.get('defaultLanguage', ''),
                'defaultAudioLanguage': video_data.get('defaultAudioLanguage', ''),
                'tags': video_data.get('tags', []),
                'viewCount': statistics.get('viewCount', '0'),
                'likeCount': statistics.get('likeCount', '0'),
                'commentCount': statistics.get('commentCount', '0'),
                'duration': content_details.get('duration', ''),
                'definition': content_details.get('definition', ''),
                'caption': content_details.get('caption', ''),
                'licensedContent': content_details.get('licensedContent', False),
                'projection': content_details.get('projection', ''),
                'url': f"https://www.youtube.com/watch?v={video_id}",
                'formattedViewCount': format_view_count(statistics.get('viewCount', '0')),
                'formattedDuration': format_duration(content_details.get('duration', ''))
            }
            videos.append(video_info)
            processed_video_ids.add(video_id)  # è¨˜éŒ„å·²è™•ç†çš„å½±ç‰‡
        
        # æ ¹æ“šè§€çœ‹æ¬¡æ•¸æ’åºä¸¦é™åˆ¶çµæœæ•¸é‡
        videos.sort(key=lambda x: int(x['viewCount']), reverse=True)
        videos = videos[:max_results]  # ç¢ºä¿è¿”å›è«‹æ±‚çš„çµæœæ•¸é‡
        
        print(f"âœ… ç¯©é¸å¾Œç²å¾— {len(videos)} å€‹ç¬¦åˆæ¢ä»¶çš„å½±ç‰‡")

        # å¦‚æœçµæœæ•¸é‡ä¸è¶³ï¼Œå˜—è©¦æ”¾å¯¬æ¢ä»¶
        if len(videos) < max_results // 2:  # å¦‚æœçµæœå°‘æ–¼è¦æ±‚çš„ä¸€åŠï¼Œå•Ÿå‹•æ”¾å¯¬æ¨¡å¼
            print(f"ğŸ”„ çµæœæ•¸é‡ {len(videos)} å°‘æ–¼è¦æ±‚çš„ä¸€åŠï¼Œå˜—è©¦æ”¾å¯¬æœå°‹æ¢ä»¶...")
            
            # å˜—è©¦æ”¾å¯¬é•·åº¦é™åˆ¶ï¼Œä½†ä¿æŒæœ€ä½è§€çœ‹æ¬¡æ•¸è¦æ±‚
            relaxed_videos = []
            relaxed_video_ids = set()  # ç”¨æ–¼å»é‡

            for item in all_video_items:
                video_data = item['snippet']
                statistics = item['statistics']
                content_details = item['contentDetails']
                video_id = item['id']

                # æª¢æŸ¥æ˜¯å¦å·²è™•ç†éæ­¤å½±ç‰‡ï¼ˆå»é‡ï¼‰
                if video_id in relaxed_video_ids:
                    continue

                # ä»ç„¶æª¢æŸ¥è§€çœ‹æ¬¡æ•¸ï¼Œä½†æ”¾å¯¬é•·åº¦é™åˆ¶
                view_count = int(statistics.get('viewCount', 0))
                if view_count < min_views:
                    continue  # è·³éè§€çœ‹æ¬¡æ•¸ä¸è¶³çš„å½±ç‰‡

                category_id = video_data.get('categoryId', '')
                category_name = get_category_name(category_id, categories)
                
                video_info = {
                    'videoId': video_id,
                    'title': video_data['title'],
                    'description': video_data['description'],
                    'channelTitle': video_data['channelTitle'],
                    'channelId': video_data['channelId'],
                    'publishedAt': video_data['publishedAt'],
                    'thumbnails': video_data['thumbnails'],
                    'categoryId': category_id,
                    'categoryName': category_name,
                    'defaultLanguage': video_data.get('defaultLanguage', ''),
                    'defaultAudioLanguage': video_data.get('defaultAudioLanguage', ''),
                    'tags': video_data.get('tags', []),
                    'viewCount': statistics.get('viewCount', '0'),
                    'likeCount': statistics.get('likeCount', '0'),
                    'commentCount': statistics.get('commentCount', '0'),
                    'duration': content_details.get('duration', ''),
                    'definition': content_details.get('definition', ''),
                    'caption': content_details.get('caption', ''),
                    'licensedContent': content_details.get('licensedContent', False),
                    'projection': content_details.get('projection', ''),
                    'url': f"https://www.youtube.com/watch?v={video_id}",
                    'formattedViewCount': format_view_count(statistics.get('viewCount', '0')),
                    'formattedDuration': format_duration(content_details.get('duration', ''))
                }
                relaxed_videos.append(video_info)
                relaxed_video_ids.add(video_id)  # è¨˜éŒ„å·²è™•ç†çš„å½±ç‰‡
            
            relaxed_videos.sort(key=lambda x: int(x['viewCount']), reverse=True)
            
            # ç¢ºä¿è¿”å›è«‹æ±‚çš„çµæœæ•¸é‡
            relaxed_videos.sort(key=lambda x: int(x['viewCount']), reverse=True)
            relaxed_videos = relaxed_videos[:max_results]  # é™åˆ¶æ”¾å¯¬æ¨¡å¼çš„çµæœæ•¸é‡
            
            if relaxed_videos:
                print(f"âœ… æ”¾å¯¬æ¢ä»¶å¾Œæ‰¾åˆ° {len(relaxed_videos)} å€‹å½±ç‰‡")
                
                # åˆä½µåŸæœ‰çµæœå’Œæ”¾å¯¬æ¢ä»¶çš„çµæœ
                existing_ids = {v['videoId'] for v in videos}
                additional_videos = [v for v in relaxed_videos if v['videoId'] not in existing_ids]
                combined_videos = videos + additional_videos
                combined_videos = combined_videos[:max_results]  # ç¢ºä¿ä¸è¶…éè«‹æ±‚æ•¸é‡
                
                # å„²å­˜åˆä½µå¾Œçš„çµæœ
                last_search_results = combined_videos.copy()
                
                # ä½¿ç”¨çœŸå¯¦çš„é…é¡è¿½è¹¤
                quota_info = get_current_quota_info()
                quota_info['video_count'] = len(combined_videos)
                
                return jsonify({
                    'success': True,
                    'videos': combined_videos,
                    'totalResults': len(combined_videos),
                    'relaxed': True,
                    'message': f'å·²æ”¾å¯¬é•·åº¦é™åˆ¶ï¼Œæ‰¾åˆ° {len(combined_videos)} å€‹å½±ç‰‡',
                    'quota_info': quota_info,
                    'can_export': len(combined_videos) > 0
                })
        
        # æ ¹æ“šè§€çœ‹æ¬¡æ•¸æ’åº
        videos.sort(key=lambda x: int(x['viewCount']), reverse=True)
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°å½±ç‰‡ï¼Œå˜—è©¦æ”¾å¯¬æ¢ä»¶
        if len(videos) == 0 and (min_views > 0 or max_duration != 'all' or time_filter != 'all'):
            print("ğŸ”„ æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„å½±ç‰‡ï¼Œå˜—è©¦æ”¾å¯¬æœå°‹æ¢ä»¶...")
            
            # é‡æ–°æœå°‹ï¼Œæ”¾å¯¬æ¢ä»¶
            relaxed_params = search_params.copy()
            if 'publishedAfter' in relaxed_params:
                del relaxed_params['publishedAfter']  # ç§»é™¤æ™‚é–“é™åˆ¶
            
            relaxed_response = youtube.search().list(**relaxed_params).execute()
            
            relaxed_video_ids = []
            for item in relaxed_response['items']:
                relaxed_video_ids.append(item['id']['videoId'])
            
            if relaxed_video_ids:
                relaxed_videos_response = youtube.videos().list(
                    part='snippet,statistics,contentDetails',
                    id=','.join(relaxed_video_ids)
                ).execute()
                
                for item in relaxed_videos_response['items']:
                    video_data = item['snippet']
                    statistics = item['statistics']
                    content_details = item['contentDetails']
                    
                    # åªæª¢æŸ¥é•·åº¦é™åˆ¶ï¼Œæ”¾å¯¬è§€çœ‹æ¬¡æ•¸è¦æ±‚
                    if max_duration != 'all':
                        duration_seconds = get_duration_seconds(content_details.get('duration', ''))
                        max_duration_seconds = int(max_duration)
                        if duration_seconds > max_duration_seconds:
                            continue
                    
                    video_info = {
                        'videoId': item['id'],
                        'title': video_data['title'],
                        'description': video_data['description'],
                        'channelTitle': video_data['channelTitle'],
                        'channelId': video_data['channelId'],
                        'publishedAt': video_data['publishedAt'],
                        'thumbnails': video_data['thumbnails'],
                        'categoryId': video_data.get('categoryId', ''),
                        'defaultLanguage': video_data.get('defaultLanguage', ''),
                        'defaultAudioLanguage': video_data.get('defaultAudioLanguage', ''),
                        'tags': video_data.get('tags', []),
                        'viewCount': statistics.get('viewCount', '0'),
                        'likeCount': statistics.get('likeCount', '0'),
                        'commentCount': statistics.get('commentCount', '0'),
                        'duration': content_details.get('duration', ''),
                        'definition': content_details.get('definition', ''),
                        'caption': content_details.get('caption', ''),
                        'licensedContent': content_details.get('licensedContent', False),
                        'projection': content_details.get('projection', ''),
                        'url': f"https://www.youtube.com/watch?v={item['id']}",
                        'formattedViewCount': format_view_count(statistics.get('viewCount', '0')),
                        'formattedDuration': format_duration(content_details.get('duration', ''))
                    }
                    videos.append(video_info)
                
                videos.sort(key=lambda x: int(x['viewCount']), reverse=True)
                
                return jsonify({
                    'success': True,
                    'videos': videos,
                    'totalResults': len(videos),
                    'relaxed': True,
                    'message': 'å·²æ”¾å¯¬æœå°‹æ¢ä»¶ä»¥é¡¯ç¤ºæ›´å¤šçµæœ'
                })
        
        # å„²å­˜æœå°‹çµæœä»¥ä¾›åŒ¯å‡ºä½¿ç”¨
        last_search_results = videos.copy()
        
        # ä½¿ç”¨çœŸå¯¦çš„é…é¡è¿½è¹¤
        quota_info = get_current_quota_info()
        quota_info['video_count'] = len(videos)  # æ›´æ–°å½±ç‰‡æ•¸é‡
        
        return jsonify({
            'success': True,
            'videos': videos,
            'totalResults': len(videos),
            'quota_info': quota_info,
            'can_export': len(videos) > 0
        })
    
    except ValueError as ve:
        return jsonify({'error': str(ve)}), 400
    except Exception as e:
        error_msg = str(e)
        if 'API key not valid' in error_msg:
            return jsonify({
                'error': 'API Key ç„¡æ•ˆ',
                'details': [
                    'è«‹æª¢æŸ¥ä»¥ä¸‹é …ç›®ï¼š',
                    '1. API Key æ ¼å¼æ­£ç¢º (ä»¥ AIzaSy é–‹é ­ï¼Œ39å­—ç¬¦)',
                    '2. å·²åœ¨ Google Cloud Console å•Ÿç”¨ YouTube Data API v3',
                    '3. API Key æœ‰æ­£ç¢ºçš„æ¬Šé™è¨­å®š',
                    '4. æœªè¶…éæ¯æ—¥é…é¡é™åˆ¶'
                ]
            }), 400
        elif 'quotaExceeded' in error_msg:
            return jsonify({'error': 'API é…é¡å·²ç”¨å®Œï¼Œè«‹æ˜å¤©å†è©¦æˆ–å‡ç´šé…é¡'}), 429
        else:
            return jsonify({'error': f'æœå°‹å¤±æ•—: {error_msg}'}), 500

@app.route('/export_csv', methods=['GET'])
def export_csv():
    """åŒ¯å‡ºCSVæª”æ¡ˆ"""
    try:
        global last_search_results, last_search_params
        
        if not last_search_results:
            return jsonify({'error': 'æ²’æœ‰å¯åŒ¯å‡ºçš„æœå°‹çµæœ'}), 400
        
        print(f"ğŸ“Š é–‹å§‹åŒ¯å‡ºCSVï¼Œå…± {len(last_search_results)} ç­†è³‡æ–™")
        print(f"ğŸ” æœå°‹åƒæ•¸: {last_search_params}")  # èª¿è©¦ç”¨
        
        # æº–å‚™CSVæ•¸æ“š - ä½¿ç”¨ BytesIO å’Œ UTF-8 ç·¨ç¢¼
        from io import BytesIO
        output = BytesIO()
        
        # å¯«å…¥ UTF-8 BOMï¼Œè®“ Excel èƒ½æ­£ç¢ºè­˜åˆ¥ç·¨ç¢¼
        output.write('\ufeff'.encode('utf-8'))
        
        # å¯«å…¥æ¨™é¡Œè¡Œ
        headers = [
            'å½±ç‰‡ID', 'å½±ç‰‡æ¨™é¡Œ', 'é »é“åç¨±', 'é »é“ID', 'å½±ç‰‡é¡åˆ¥', 'ä¸Šå‚³æ™‚é–“',
            'è§€çœ‹æ¬¡æ•¸', 'æŒ‰è®šæ•¸', 'ç•™è¨€æ•¸', 'å½±ç‰‡é•·åº¦', 'ç•«è³ª', 'å­—å¹•', 
            'æˆæ¬Šå…§å®¹', 'å½±ç‰‡é€£çµ', 'å½±ç‰‡æè¿°', 'æ¨™ç±¤'
        ]
        
        # å»ºç«‹ CSV è¡Œ
        csv_lines = []
        csv_lines.append(','.join([f'"{h}"' for h in headers]))
        
        # å¯«å…¥æ•¸æ“šè¡Œ
        for video in last_search_results:
            try:
                # è™•ç†æ™‚é–“æ ¼å¼
                try:
                    published_date = datetime.fromisoformat(video['publishedAt'].replace('Z', '+00:00'))
                    formatted_date = published_date.strftime('%Y-%m-%d %H:%M:%S')
                except:
                    formatted_date = video['publishedAt']
                
                # è™•ç†æ¨™ç±¤
                tags_str = ', '.join(video.get('tags', [])) if video.get('tags') else 'ç„¡'
                
                # æ¸…ç†æè¿°æ–‡å­—ä¸­çš„æ›è¡Œç¬¦å’Œç‰¹æ®Šå­—ç¬¦
                description = video['description'].replace('\n', ' ').replace('\r', ' ').replace('"', '""')
                if len(description) > 500:
                    description = description[:500] + '...'
                
                # æ¸…ç†æ¨™é¡Œä¸­çš„ç‰¹æ®Šå­—ç¬¦
                title = video['title'].replace('"', '""')
                channel_title = video['channelTitle'].replace('"', '""')
                
                row_data = [
                    video['videoId'],
                    title,
                    channel_title,
                    video['channelId'],
                    video.get('categoryName', 'æœªçŸ¥'),
                    formatted_date,
                    str(int(video['viewCount'])),
                    str(int(video['likeCount'])),
                    str(int(video['commentCount'])),
                    video['formattedDuration'],
                    video['definition'],
                    'æœ‰' if video['caption'] == 'true' else 'ç„¡',
                    'æ˜¯' if video['licensedContent'] else 'å¦',
                    video['url'],
                    description,
                    tags_str
                ]
                
                # å°‡æ¯å€‹æ¬„ä½åŒ…è£¹åœ¨å¼•è™Ÿä¸­ä»¥è™•ç†é€—è™Ÿå’Œç‰¹æ®Šå­—ç¬¦
                csv_line = ','.join([f'"{field}"' for field in row_data])
                csv_lines.append(csv_line)
            except Exception as e:
                print(f"âš ï¸ è™•ç†å½±ç‰‡ {video.get('videoId', 'unknown')} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue
        
        # å¯«å…¥æ‰€æœ‰è¡Œ
        csv_content = '\n'.join(csv_lines)
        output.write(csv_content.encode('utf-8'))
        
        # æº–å‚™ä¸‹è¼‰
        output.seek(0)
        
        # æ ¹æ“šæœå°‹æ¢ä»¶ç”Ÿæˆæª”æ¡ˆåç¨±ï¼ˆæ”¹ç‚º CSVï¼‰
        filename = generate_csv_filename(last_search_params)
        
        print(f"âœ… CSVæª”æ¡ˆç”ŸæˆæˆåŠŸ: {filename}ï¼Œå…± {len(csv_lines)-1} ç­†è³‡æ–™")
        
        # å‰µå»ºéŸ¿æ‡‰ - ä½¿ç”¨ send_file è™•ç†äºŒé€²åˆ¶æ•¸æ“š
        from flask import send_file
        from urllib.parse import quote
        
        output.seek(0)
        
        # URL ç·¨ç¢¼æª”åä»¥æ”¯æ´ä¸­æ–‡
        encoded_filename = quote(filename)
        
        return send_file(
            output,
            mimetype='text/csv',
            as_attachment=True,
            download_name=filename
        )
        
    except Exception as e:
        print(f"âŒ CSVåŒ¯å‡ºéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'åŒ¯å‡ºå¤±æ•—: {str(e)}'}), 500

if __name__ == '__main__':
    print("=" * 50)
    print("YouTube ç†±é–€å½±ç‰‡æœå°‹å™¨")
    print("=" * 50)
    
    api_key = get_api_key()
    
    if not api_key or api_key == 'your_youtube_api_key_here':
        print("âŒ éŒ¯èª¤: è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®šæ‚¨çš„ YouTube API Key")
        print("ğŸ“ æ­¥é©Ÿ:")
        print("   1. å‰å¾€ https://console.cloud.google.com/")
        print("   2. å»ºç«‹æˆ–é¸æ“‡å°ˆæ¡ˆ")
        print("   3. å•Ÿç”¨ YouTube Data API v3")
        print("   4. å»ºç«‹æ†‘è­‰ > API Key")
        print("   5. ç·¨è¼¯ .env æª”æ¡ˆ: YOUTUBE_API_KEY=æ‚¨çš„APIé‡‘é‘°")
        print()
    elif not api_key.startswith('AIzaSy') or len(api_key) != 39:
        print(f"âš ï¸  è­¦å‘Š: API Key æ ¼å¼å¯èƒ½ä¸æ­£ç¢º")
        print(f"   ç›®å‰çš„ Key: {api_key[:10]}...")
        print(f"   YouTube API Key æ‡‰è©²ä»¥ 'AIzaSy' é–‹é ­ä¸”é•·åº¦ç‚º 39 å­—ç¬¦")
        print()
    else:
        print("âœ… API Key æ ¼å¼çœ‹èµ·ä¾†æ­£ç¢º")
        print()
    
    print("ğŸš€ å•Ÿå‹•ä¼ºæœå™¨...")
    app.run(debug=True, host='0.0.0.0', port=5000)